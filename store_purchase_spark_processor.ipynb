{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+---------------+----------------+----------------+----------------+\n",
      "|      DATE|      CUSTOMER_UUID|     SHOP|PURCHASE_AMOUNT|   PURCHASE_TIME|   TIME_OF_ENTRY|     TIME_OF_OUT|\n",
      "+----------+-------------------+---------+---------------+----------------+----------------+----------------+\n",
      "|2017-10-10|abc456-46789-123456|   Auchan|         125.56|2017-10-10 10:45|2017-10-10 10:00|2017-10-10 11:00|\n",
      "|2017-10-10|abc456-46789-123456|Carrefour|          82.75|2017-10-10 16:10|2017-10-10 16:00|2017-10-10 16:30|\n",
      "|2017-10-10|def456-46789-123456|   Auchan|              0|                |2017-10-10 13:00|2017-10-10 15:00|\n",
      "|2017-10-10|rtg456-46789-123456|   Auchan|              0|                | 2017-10-10 8:00| 2017-10-10 9:30|\n",
      "+----------+-------------------+---------+---------------+----------------+----------------+----------------+\n",
      "\n",
      "Parquet file generated with name ===> store_purchase_analysis_2017-10-10.parquet\n",
      "Most popular shop of the day ===> Auchan\n"
     ]
    }
   ],
   "source": [
    "# ---- author - Aniruddha Anikhindi ------\n",
    "# Program for data analysis for retail store and purchase log analysis\n",
    "#Reason for using spark - possibility of million of records in logs \n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as Fun\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "\n",
    "#sc - spark context - Enamble below 2 lines when loadig first time\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "#Here we assume log files are ordered for Entry-Leave data\n",
    "#If log file contains unordered data - then it should be grouped first with customeruuid,shopname and then by ascending order of timestamp\n",
    "\n",
    "store_event_file_path = r'StoreEvent_2017-10-10.csv'\n",
    "purchase_event_file_path = r'Purchases_2017-10-10.csv'\n",
    "parquet_file_name = r'store_purchase_analysis_2017-10-10.parquet'\n",
    "\n",
    "raw_rdd_se = sc.textFile(store_event_file_path)\n",
    "headers_se = raw_rdd_se.first()\n",
    "rdd_se_without_header = raw_rdd_se.filter(lambda x: x != headers_se)\n",
    "rdd_se_splitted = rdd_se_without_header.map(lambda l: l.split(\",\"))\n",
    "rdd_store_events = rdd_se_splitted.map(lambda p: Row(date_se=p[0], custuuid=str(p[1]).strip(), eventname=str(p[2]).strip(), \n",
    "                                   shopname=str(p[3]).strip(), timestamp_se= str(p[4]).strip()))\n",
    "df_se = sql_context.createDataFrame(rdd_store_events)\n",
    "df_grouped_se = df_se.groupBy(\"custuuid\",\"shopname\").agg(Fun.max(\"date_se\").alias('date_se'), \n",
    "                                                   Fun.min(\"timestamp_se\").alias('from_ts'),\n",
    "                                                   Fun.max(\"timestamp_se\").alias('to_ts')).orderBy('custuuid', ascending=True)\n",
    "df_grouped_se.registerTempTable('SE')\n",
    "\n",
    "\n",
    "raw_rdd_pe = sc.textFile(purchase_event_file_path)\n",
    "headers_pe = raw_rdd_pe.first()\n",
    "rdd_pe_without_header = raw_rdd_pe.filter(lambda x: x != headers_pe)\n",
    "rdd_pe_splitted = rdd_pe_without_header.map(lambda l: l.split(\",\"))\n",
    "rdd_purchase_events = rdd_pe_splitted.map(lambda p: Row(date_pe=p[0], custuuid=str(p[1]).strip(), amount=p[2], \n",
    "                                                   ts_pe=str(p[3]).strip(),ts_str=str(p[3]).strip()))\n",
    "df_pe = sql_context.createDataFrame(rdd_purchase_events)\n",
    "df_pe.registerTempTable('PE')\n",
    "\n",
    "sql = r'SELECT se.date_se as DATE, se.custuuid as CUSTOMER_UUID, se.shopname as SHOP, IF(pe.amount is not null,pe.amount,0) as PURCHASE_AMOUNT, if(pe.ts_str is not null,pe.ts_str,\"\") as PURCHASE_TIME, se.from_ts as TIME_OF_ENTRY, se.to_ts as TIME_OF_OUT FROM SE se FULL OUTER JOIN PE pe ON  cast(se.from_ts as timestamp) <= cast(pe.ts_pe as timestamp) and cast(se.to_ts as timestamp) >= cast(pe.ts_pe as timestamp)'\n",
    "df_sql = sql_context.sql(sql)\n",
    "df_sql.show()\n",
    "\n",
    "#df_final = df_sql.fillna( { 'PURCHASE_AMOUNT':0, 'PURCHASE_TIME':'' } ) --  One more option replace null with userdefined values\n",
    "\n",
    "df_sql.write.mode('overwrite').parquet(parquet_file_name)\n",
    "print('Parquet file generated with name ===> '+parquet_file_name)\n",
    "\n",
    "df_gropued_shop = df_final.groupBy('SHOP').count().orderBy('count',ascending=False)\n",
    "popular_shop = df_gropued_shop.toPandas().loc[0]['SHOP']\n",
    "print('Most popular shop of the day ===> '+popular_shop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
